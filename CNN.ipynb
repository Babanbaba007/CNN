{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model input data loading, train, and evaluate\n",
    "This jupyter notebook shows how preprocessed input datas be further processed as final input form then, train and evaluate.\n",
    "\n",
    "CNN model from https://github.com/IcarPA-TBlab/MetagenomicDC/blob/master/models/CNN.py\n",
    "\n",
    "This model is used in the paper from Fiannaca (https://doi.org/10.1186/s12859-018-2182-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.init(project=\"CNN\", entity=\"bachelorprojectgroup9\",settings=wandb.Settings(start_method=\"fork\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    lista = []\n",
    "    records = list(open(file, \"r\"))\n",
    "    records = records[1:]\n",
    "    for seq in records:\n",
    "        elements = seq.split(\",\")\n",
    "        level = elements[-1].split(\"\\n\")\n",
    "        classe = level[0]\n",
    "        lista.append(classe)\n",
    "\n",
    "    #make taxon list\n",
    "    lista = set(lista)\n",
    "    classes = list(lista)\n",
    "    X = []\n",
    "    Y = []\n",
    "    for seq in records:\n",
    "        elements = seq.split(\",\")\n",
    "        X.append(elements[1:-1])\n",
    "        level = elements[-1].split(\"\\n\")\n",
    "        classe = level[0]\n",
    "        Y.append(classes.index(classe))\n",
    "    X = np.array(X, dtype=float)\n",
    "    Y = np.array(Y, dtype=int)\n",
    "    data_max = np.amax(X)\n",
    "    X = X / data_max\n",
    "    return  X, Y, classes, len(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading process\n",
    "\n",
    "input data contains k-mer matrix, k-mer frequency table, and corresponding taxon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22222222 0.22222222 0.         ... 0.11111111 0.11111111 0.        ]\n",
      " [0.         0.         0.11111111 ... 0.         0.         0.        ]\n",
      " [0.         0.11111111 0.11111111 ... 0.         0.22222222 0.22222222]]\n",
      "[0 1 2]\n",
      "['Eubacteriales', 'Rhodospirillales', 'Bacteroidales']\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "X, Y, classes, input_len =load_data('/Users/jihwanlim/Desktop/G9_CNN/Sample.txt')\n",
    "print(X)\n",
    "print(Y)\n",
    "print(classes)\n",
    "print(input_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data (X) is shown in above. each value of k-mer frequency table is divided with whth the maximum value of table for normalization.\n",
    "\n",
    "(Y)refers to the labeling of corressponding each input data and classes shows the name of label, taxon name.\n",
    "\n",
    "Input length depends on the k value of k-mer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(nb_classes, input_length):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(5, 5, padding='valid', input_shape=(input_length,1)))  # input_dim\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, padding='valid'))\n",
    "    model.add(Convolution1D(10, 5, padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, padding='valid'))\n",
    "    model.add(Flatten())\n",
    "    ##\n",
    "    ##MLP\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, datatr, labelstr, dataval, labelsval, nb_classes):\n",
    "    datatr = datatr.reshape(datatr.shape + (1,))\n",
    "    labelstr = np_utils.to_categorical(labelstr, nb_classes)\n",
    "\n",
    "    dataval = dataval.reshape(dataval.shape + (1,))\n",
    "    labelsval = np_utils.to_categorical(labelsval, nb_classes)\n",
    "\n",
    "    print ('Fitting model...')\n",
    "    model_fit = model.fit(datatr, labelstr, epochs=30, batch_size=512, verbose=1, validation_data = (dataval, labelsval),callbacks=[WandbCallback()])\n",
    "\n",
    "    tr_scores = model.evaluate(datatr, labelstr, verbose=1)\n",
    "\n",
    "    return model_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data is rearragend into (num_dataset, input_length, 1) numpy array shape with x.reshape(x.shape + (1,))",
    "\n",
    "Labels are transformed into one hot encoding form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.22222222]\n",
      "  [0.22222222]\n",
      "  [0.        ]\n",
      "  ...\n",
      "  [0.11111111]\n",
      "  [0.11111111]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.11111111]\n",
      "  ...\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.11111111]\n",
      "  [0.11111111]\n",
      "  ...\n",
      "  [0.        ]\n",
      "  [0.22222222]\n",
      "  [0.22222222]]]\n"
     ]
    }
   ],
   "source": [
    "X_reshape = X.reshape(X.shape + (1,))\n",
    "print(X_reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, datate, labelste, nb_classes):\n",
    "\n",
    "    labelste_bin = np_utils.to_categorical(labelste, nb_classes)\n",
    "    datate = datate.reshape(datate.shape + (1,))\n",
    "\n",
    "    preds = model.predict_classes(datate, verbose=1)\n",
    "\n",
    "    score, acc = model.evaluate(datate, labelste_bin, verbose=1)\n",
    "    print('Test loss:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "    return preds, labelste_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preds value means that predeiction of test input data by CNN model.\n",
    "\n",
    "labelste_bin is the answer of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print ('Loading data...')\n",
    "\n",
    "    x_train, y_train, classes, input_length = load_data(sys.argv[2])\n",
    "\n",
    "    x_val, y_val, classes_val, val_input_length_semi = load_data(sys.argv[3])\n",
    "\n",
    "    x_test, y_test, classes_test, test_input_length_semi = load_data(sys.argv[4])\n",
    "\n",
    "    nb_classes = len(classes)\n",
    "\n",
    "    print ('Loading model...')\n",
    "    \n",
    "    model = create_model(nb_classes, len(x_train[0]))   \n",
    "    model_fit = train_model(model, x_train, y_train, x_val, y_val, nb_classes)\n",
    "    \n",
    "    model.save(\"/home/ba3-project-9/CNN/result/CNN_aa_d3_20,256.h5\")\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "    pred, Y_test = evaluate_model(model, x_test, y_test, nb_classes)\n",
    " \n",
    "\n",
    "    Y_test_decode = []\n",
    "    Y_test = Y_test.tolist()\n",
    "    for matrix in Y_test:\n",
    "        Y_test_decode.append(matrix.index(1.0))\n",
    "\n",
    "    f1 = f1_score(Y_test_decode, pred, average='weighted')\n",
    "    mcc = matthews_corrcoef(Y_test_decode, pred)\n",
    "    print('f1_score:', f1)\n",
    "    print('MCC:', mcc)\n",
    "\n",
    "    np.save(\"/home/ba3-project-9/CNN/result/pred_tanh\", pred)\n",
    "    np.save(\"/home/ba3-project-9/CNN/result/Y_test_tanh\", Y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 2.7.18 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
